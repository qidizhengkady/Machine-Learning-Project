---
title: "Retail Marketing Project"
author  : "Qidi Zheng"
date    : "03/12/2022" 
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
    highlight: tango
    df_print: paged
---


## Load Libraries 

```{r, warning=FALSE, message=FALSE}

library(C50)
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(kableExtra)
library(GGally)
library(kableExtra) # -- make nice looking resutls when we knitt 
library(vip)        # --  tidymodels variable importance
library(fastshap)   # -- shapley values for variable importance 
library(MASS)
library(rpart.plot) # -- plotting decision trees 
library(factoextra)
library(imputeMissings)
library(ISLR)
library(tree)
library(ggplot2)
library(ranger) 

options(scipen=999)
```

## Load Data


```{r}
project_original <- read_csv("DonorMerge_Final.csv") %>%
  clean_names()
project_original %>% 
  skim()

donation_original <- read.csv("Donations.csv") %>%  
  clean_names()
donation_original %>% 
  skim()
#donation_join <- donation_original %>% left_join(project_original,"projectid"="projectid")

# remove "rejected" variables from the data frame

project = subset(project_original, select= -c(projectid,teacher_acctid,schoolid,school_ncesid,school_city,school_district,school_county,school_zip,secondary_focus_subject, secondary_focus_area,date_posted,great_messages_proportion,school_longitude,students_reached,school_magnet, school_year_round,school_nlns,school_charter_ready_promise,teacher_prefix,primary_focus_subject,poverty_level,primary_focus_area,grade_level))
head(project)

donation=subset(donation_original,select = -c(donationid,projectid,donor_acctid,donor_city,donor_state,donation_timestamp,donation_message,donor_zip))
head(donation)


```

## change target to factor and Profile data


```{r}

project$is_exciting <- as.factor(project$is_exciting)

project <- project %>% mutate_if(is.character,factor) %>% mutate_if(is.logical,factor)

project %>%
  skim()

```

# Exploratory Data

```{r}
project_vis <- project %>% 
  mutate_if(is.character, factor) %>% 
  mutate_if(is.logical, factor)

for (c in names(project_vis) ) {
  if (c %in% names(project_vis %>% dplyr::select(where(is.factor)))) {
    # -- for each character column create a chart
    print( project_vis %>%
            ggplot(., aes(!!as.name(c))) + 
            geom_bar(aes(fill = factor(is_exciting))) + labs(title = c, y = "count"))
  } else {
   # -- comparative boxplots
    print(ggplot(project_vis, aes(x=is_exciting, y=!!as.name(c), fill=is_exciting))+ geom_boxplot() +labs(title = c))
  }
}

```


#prepare data for clustering-recipe

```{r}
# impute missing values

donation_filter <- donation %>% filter(donation_total<=400)
donation_recipe =recipe(~.,data = donation_filter) %>%
  step_impute_mode(all_nominal_predictors()) %>% 
  step_impute_median(all_numeric_predictors()) %>% #replace numeric missing values 
  step_novel(all_nominal_predictors()) %>%        #handle new levels 
  step_zv(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_unknown(all_nominal_predictors()) %>%       # replace category missing values
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%  # one-hot encode 
    prep()
head(donation_recipe)

donation_bake <- bake(donation_recipe %>% prep(),new_data = donation_filter)
skim(donation_bake)



```

# determine number of clusters and run kmeans

```{r}

# how many clusters do we need?


donor_sample <-sample_frac(donation_bake,0.02)


fviz_nbclust(donor_sample, kmeans, method="wss")

```




# generate and visualize clusters

```{r}
set.seed(08)


clusters1 <- kmeans(donation_bake, 5, iter.max = 200, nstart = 10) 

print(clusters1)

# visualize clusters

#fviz_cluster(clusters1,donation_bake,ellipse.type="norm",geom="point") 

```

# profile clusters

```{r}

donation_bake$cluster <- clusters1$cluster


donation_bake$cluster <- as.factor(donation_bake$cluster)

donation_bake$cluster %>%
  skim()

# review cluster sizes-not changed

ggplot(donation_filter,aes(donation_bake$cluster))+geom_bar()

ggplot(donation_filter,aes(x=is_teacher_acct))+geom_bar()
ggplot(donation_filter,aes(x=is_teacher_acct))+geom_bar() + facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(x=donation_to_project))+geom_histogram(binwidth=30)
ggplot(donation_filter,aes(x=donation_to_project))+geom_histogram(binwidth=30) + facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(x=donation_optional_support))+geom_histogram(binwidth=30)
ggplot(donation_filter,aes(x=donation_optional_support))+geom_histogram(binwidth=30) + facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(x=donation_total))+geom_histogram(binwidth=30)
ggplot(donation_filter,aes(x=donation_total))+geom_histogram(binwidth=30) + facet_wrap(~donation_bake$cluster)


ggplot(donation_filter,aes(dollar_amount))+geom_bar()
ggplot(donation_filter,aes(dollar_amount))+geom_bar()+facet_wrap(~donation_bake$cluster)


ggplot(donation_filter,aes(donation_included_optional_support))+geom_bar()
ggplot(donation_filter,aes(donation_included_optional_support))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(payment_method))+geom_bar()
ggplot(donation_filter,aes(payment_method))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(payment_included_acct_credit))+geom_bar()
ggplot(donation_filter,aes(payment_included_acct_credit))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(payment_included_campaign_gift_card))+geom_bar()
ggplot(donation_filter,aes(payment_included_campaign_gift_card))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(payment_included_web_purchased_gift_card))+geom_bar()
ggplot(donation_filter,aes(payment_included_web_purchased_gift_card))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(payment_was_promo_matched))+geom_bar()
ggplot(donation_filter,aes(payment_was_promo_matched))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(via_giving_page))+geom_bar()
ggplot(donation_filter,aes(via_giving_page))+geom_bar()+facet_wrap(~donation_bake$cluster)

ggplot(donation_filter,aes(for_honoree))+geom_bar()
ggplot(donation_filter,aes(for_honoree))+geom_bar()+facet_wrap(~donation_bake$cluster)




# donation_filter %>%
#   group_by(donation_bake$cluster) %>%
#   summarize(mean_donation_to_project = mean(donation_to_project),
#             min_donation_to_project = min(donation_to_project),
#             max_donation_to_project = max(donation_to_project),
#             mean_meat = mean(meat),
#             min_meat = min(meat),
#             max_meat = max(meat),
#             mean_catalog = mean(catalog),
#             min_catalog = min(catalog),
#             max_catalog = max(catalog),
#             mean_store = mean(store),
#             min_store = min(store),
#             max_store = max(store),
#             mean_visits = mean(visits),
#             min_visits = min(visits),
#             max_visits = max(visits)
#             )


```



## Model 1 - random forest
#create train and test data, apply recipe, build model

```{r}
# -- set a random seed for repeatablity 
set.seed(08)

# -- performs our train / test split 
project_prep <- project %>% 
  mutate_if(is.character,factor) %>%
  mutate_if(is.logical, factor) %>% 
  mutate(is_exciting  = if_else(is_exciting=='TRUE',1,0))%>% 
  mutate(is_exciting = factor(is_exciting)) 
  

project_split <- initial_split(project_prep, prop = 0.7)

# -- extract the training data 
project_train <- training(project_split)
# -- extract the test data 
project_test <- testing(project_split)

sprintf("Train PCT : %1.2f%%", nrow(project_train)/ nrow(project) * 100)
sprintf("Test  PCT : %1.2f%%", nrow(project_test)/ nrow(project) * 100)

head(project_train)


```
```{r}

rf_project_recipe <- recipe(is_exciting ~ ., data = project_train) %>%
  step_impute_median(all_numeric_predictors())%>%
  step_impute_mode(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors(),-all_outcomes()) %>% 
  step_dummy(all_nominal(),-all_outcomes()) %>%
  prep()

rf_project_recipe

# -- apply the recipe 
rf_bake_train <- bake(rf_project_recipe, new_data = project_train)
rf_bake_test  <- bake(rf_project_recipe, new_data = project_test)


```


#fit model
```{r}
 rf_model <- rand_forest(mtry=5, min_n=10, trees=500, mode = "classification") %>%
                       set_engine("ranger", importance="impurity") %>%
                    fit(is_exciting ~ ., data = rf_bake_train)


 rf_model$fit

rf_model %>%
  vip(num_features = 10)


```


## Prep for Evaluation 


```{r}

# -- training 
predict(rf_model, rf_bake_train, type = "prob") %>%
  bind_cols(.,predict(rf_model, rf_bake_train)) %>%
  bind_cols(.,rf_bake_train) -> scored_train_tree

head(scored_train_tree)

# -- testing 
predict(rf_model, rf_bake_test, type = "prob") %>%
  bind_cols(.,predict(rf_model, rf_bake_test)) %>%
  bind_cols(.,rf_bake_test) -> scored_test_tree

head(scored_test_tree)
```

## Evaluate


```{r}
options(yardstick.event_first = FALSE)

# -- AUC: Train and Test 
scored_train_tree %>% 
  metrics(is_exciting, .pred_1, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test_tree %>% 
               metrics(is_exciting, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") 
  ) 
  

# -- ROC Charts 
scored_train_tree %>%
  mutate(model = "train") %>%
  bind_rows(scored_test_tree %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(is_exciting, .pred_1) %>%
  autoplot()


# -- Confustion Matricies  
scored_train_tree %>%
  conf_mat(is_exciting, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Train Confusion Matrix")

scored_test_tree %>%
  conf_mat(is_exciting, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Test Confusion Matrix")




```


## Model 2 - Logistic Model


## 2. Recipe & Bake


```{r}
# -- create our recipe -- 
log_recipe <- recipe(is_exciting ~ ., data = project_train) %>%
  step_impute_median(all_numeric_predictors())%>%
  step_impute_mode(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors(),-all_outcomes()) %>% 
  step_dummy(all_nominal(),-all_outcomes()) %>%
  prep()

    
log_recipe

# -- apply the recipe 
bake_log_train <- bake(log_recipe, new_data = project_train)
bake_log_test  <- bake(log_recipe, new_data = project_test)

```


## 3. Fit 

Now we are ready to fit our model. Notice that you are creating a model object (logistic_glm) by calling the logistic_reg method, specifying the mode classification since we are creating a classification task, you set the engine to which engine you want to use typically glm or glmnet then you specify the formula in the fit method and point to your baked data. 


```{r,warning=FALSE, message=FALSE}
logistic_glm <-logistic_reg(mode = "classification") %>%
                  set_engine("glm") %>%
                  fit(is_exciting ~ ., data = bake_log_train)


## -- check out your parameter estimates ... 
tidy(logistic_glm) %>%
  mutate_at(c("estimate", "std.error", "statistic", "p.value"),round, 4)





```

## 4. Prep for Evaluation 

We want to attach both the Predicted Probabilities (.pred_No, .pred_Yes) and the Predicted Class (.pred_class) to the dataset so we can deep dive into where out model is performing well and where it's not. We do this to both the Training and the Test set. 

```{r}

# -- training 
predict(logistic_glm, bake_log_train, type = "prob") %>%
  bind_cols(.,predict(logistic_glm, bake_log_train)) %>%
  bind_cols(.,bake_log_train) -> scored_train_glm

head(scored_train_glm)

# -- testing 
predict(logistic_glm, bake_log_test, type = "prob") %>%
  bind_cols(.,predict(logistic_glm, bake_log_test)) %>%
  bind_cols(.,bake_log_test) -> scored_test_glm

head(scored_test_glm)
```

## 5. Evaluate

We want to check our model's performance and take a look at which features were most important. 

```{r}
options(yardstick.event_first = FALSE)
# -- AUC: Train and Test 
scored_train_glm %>% 
  metrics(is_exciting, .pred_1, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test_glm %>% 
               metrics(is_exciting, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") 
  ) 


# -- Variable Importance top 10 features  
logistic_glm %>%
  vip(num_features = 10)

# -- ROC Charts 
scored_train_glm %>%
  mutate(model = "train") %>%
  bind_rows(scored_test_glm %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(is_exciting, .pred_1) %>%
  autoplot()


# -- Confustion Matricies  
scored_train_glm %>%
  conf_mat(is_exciting, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Train Confusion Matrix")

scored_test_glm %>%
  conf_mat(is_exciting, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Test Confusion Matrix")
```


